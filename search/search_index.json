{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Skyway Documentation","text":"<p>Skyway is an integrated platform developed at the RCC to allow users to burst computing workloads from the on-premise RCC cluster, Midway, to run on remote commercial cloud platforms such as Amazon AWS, Google GCP and Microsoft Azure. Skyway enables users to run computing tasks in the cloud from Midway in a seamless manner without needing to learn how to provision cloud resources. Since the user does not need to setup or manage cloud resources themselves, the result is improved productivity with a minimum learning curve.</p> <p>The official Skyway homepage gives useful information for getting started.</p>"},{"location":"#overview","title":"Overview","text":"<p>From the user persepectives, Skyway functions in a similar way to a HPC cluster where they can transfer data in and out, compile software or load existing modules, submit jobs either to on-premises compute nodes, or to virtual machines (aka instances) from a cloud service provider. Skyway uses SLURM as a resource manager in the cloud. Resources in the cloud have the same configuration, software modules and file storage systems as Midway. The User Guide provides more information.</p> <p>From the admin perspectives, Skyway relies on SLURM to burst jobs to the corresponding cloud accounts and partitions given the requested instance features. Although Skyway manages user accounts and billing and monitors jobs, it relies on how the University IT Services support the secured connections between the cloud accounts and the Skyway login node.</p> <p>After downloading and installing the Skyway package into root-access places, admins add configuration files, cloud services and accounts, and launch the daemon for individual accounts. The daemons run in the background periodically checks the SLURM queue for jobs submitted to the cloud, and instantiate (or activate) the virtual machines on the cloud given the user credential and the account specified in the job script. More information is given in the Developer Guide.</p> <p>The Developer Guide also serves to provide information on how to add more functionalities to Skyway.</p> Documentation Description Audience User Guide Installing pre-requisite software and acquiring source code End users Developer Guide Adding cloud services and starting the daemons Admins and developers"},{"location":"#where-to-start","title":"Where to start?","text":"<ul> <li>Researchers interested in using the RCC systems can request an account.  </li> <li>For Service Units (computing time) and storage resources, request an allocation.  </li> <li>If you would like to chat with an RCC specialist about what services are best for you, please email help@rcc.uchicago.edu</li> </ul>"},{"location":"developer_overview/","title":"Developer Guide","text":"<p>This documentation provides information for admins and developers to install and deploy Skyway on a management node and login node. </p>"},{"location":"developer_overview/#pre-requisites","title":"Pre-requisites","text":"<p>The login node and service (management) node should be installed the following software:</p> <ul> <li>SLURM</li> <li>NFS</li> </ul> <p>The following <code>python</code> packages are also needed to be ready to use</p> <ul> <li>miniconda</li> <li>boto3</li> <li>libcloud</li> <li>pysql</li> <li>tabulate</li> </ul> <p>The login and management nodes are set up in the same manner as for a regular login node and SLURM management node, ready for accepting jobs. If users submit jobs to on-premises compute nodes (e.g. by specifying a proper partition), the jobs will be put on a regular queue.</p>"},{"location":"developer_overview/#installation-and-configuration","title":"Installation and Configuration","text":"<p>On the (management) node (e.g. <code>skyway-dev</code> that is accessible from the <code>midway3</code> login node):</p> <ol> <li>Create a folder named as <code>skyway</code> at root: <code>/opt/skyway</code></li> <li>Checkout this repo into <code>/opt/skyway/pkgs/skyway</code> ((or later <code>pip install skyway-cloud</code> via PyPI)</li> <li>Prepare a configuration folder <code>/opt/skyway/etc/</code></li> <li> <p>Prepare several configuration files under <code>/opt/skyway/etc</code></p> <ul> <li><code>root.yaml</code>: contains any attributes used by the management process.</li> <li><code>cloud.yaml</code>: contains global information for every cloud vendors and VM types (for all customers, note the differences between AWS and GCP).</li> <li><code>skyway.yaml</code>: defines configuration for the <code>skyway</code> python package</li> <li><code>accounts/*.yaml</code>: defines configuration for specific cloud accounts (see the .yaml files for setting up individual PIs and RCC's cloud accounts)</li> <li><code>services/*.yaml</code>: defines a service daemon process: The file name contains account (i.e. group) and partition in the format of <code>[group]-[cloud]</code>, for example, <code>cloud.rcc-aws.yaml</code></li> </ul> <p>Below are some examples of the configuration files:</p> </li> </ol> root.xmlcloud.xmlskyway.xmlservices/cloud.rcc-aws.yamlaccounts/rcc-aws.yml <pre><code>email: root@skyway-dev.rcc.uchicago.edu\n</code></pre> <pre><code>aws:\n    master_access_key_id: 'some-string'\n    master_secret_access_key: '[another-string]'\n\n    username: centos\n    ami-id : ami-[id-string]\n    key-name: rcc-skyway\n    grace_sec: 300\n\n    node-types:\n        t1:  { name: t2.micro,    price: 0.0116, cores: 1,  memgb: 0.8 }\n        c1:  { name: c5.large,    price: 0.085,  cores: 1,  memgb: 3.5 }\n        c8:  { name: c5.4xlarge,  price: 0.68,   cores: 8,  memgb: 32 }\n    ...\n\ngcp:\n</code></pre> <pre><code>paths:\n    etc: &lt;ROOT&gt;/etc/\n    var: &lt;ROOT&gt;/var/\n    run: &lt;ROOT&gt;/run/\n    log: &lt;ROOT&gt;/log/\n    files: &lt;ROOT&gt;/files/\n\ndb:\n    host: localhost\n    port: 3306\n    username: skyway\n    password: \"the-password\"\n    database: skyway\n</code></pre> <pre><code>module: cloud\nkwargs:\n  account: rcc-aws\nevery: 15\nactive: No\n</code></pre> <pre><code>cloud: aws\ngroup: rcc\n\naccount:\n  account_id: [account-id]\n  role_name: rcc-skyway\n  region: us-east-2\n  security_group: ['sg-[some-string]']\n  protected_nodes: []\n\nnodes:\n  t1: 8\n\nusers:\n  - [user-name1]\n  - [user-name2]\n</code></pre> <p>Finally, prepare the following script under <code>/opt/skyway/bin</code>, named <code>skyway</code>:</p> <pre><code>#!/bin/sh\n\nexport SKYWAYROOT=/skyway\nexport PYTHONPATH=$PYTHONPATH:/skyway/pkgs\n\nif [ \"`which python3 2&gt;/dev/null`\" = \"\" ]; then source /skyway/bin/bashrc; fi\n\nif [ \"$*\" = \"\" ]; then python3 -m skyway\nelse python3 -m skyway.$*; fi\n</code></pre> <p>and give it execute-mode <pre><code>chmod +x /opt/skyway/bin/skyway\n</code></pre></p> <p>On the head node (e.g. <code>skyway2-login1</code>):</p> <ul> <li>What should I do on the skyway login node so that users can submit jobs through skyway? \u2013 No need to do anything\u2013 just to make sure that SLURM functions normally.</li> </ul>"},{"location":"developer_overview/#deployment","title":"Deployment","text":"<ul> <li>On the service node? How to launch the daemon? \u2013 see above: each cloud partition (or account) needs a separate process (daemon) to be launched via <code>skyway service [name-of-the-partition]</code>: a configuration file is expected under <code>/opt/skyway/etc/services</code>, for example, <code>cloud.rcc-aws.yaml</code> for the RCC account with AWS. There is also a so-called <code>watcher</code> (defined by <code>watcher.yaml</code>) that needs to be launched to overlook all cloud services.</li> <li>On the login node? No need to do anything, users just submit jobs to SLURM, specifying the constraint (e.g. <code>c4</code> or <code>g1</code>) and their account, then the <code>skyway</code> watcher on the management node will periodically query the SLURM queue to send the jobs to the cloud partition.</li> </ul>"},{"location":"developer_overview/#common-commands","title":"Common commands","text":"<p>The following common commands are available on the service node.</p> <pre><code>skyway service\nskyway service --status\nskyway service --regist billing\nskyway service --restart billing\nskyway service --restart cloud-rcc-aws\nskyway service --start cloud-rcc-aws\nskyway service --stop cloud-rcc-aws\n\nskyway cloud\nskyway cloud rcc-aws --test\nskyway cloud rcc-aws --connect rcc-aws-t1-001\nskyway cloud rcc-aws --connect rcc-io\nskyway cloud rcc-aws --ls\nskyway cloud rcc-aws --rm i-0ecb224c29fdcb688\n\nskyway billing\nskyway billing rcc-aws --set amount=10\nskyway billing rcc-aws --set rate=6.0\nskyway billing rcc-aws --summary\n\nskyway misc.db_test\nskyway misc.nodes\nskyway misc.nodes --update\nskyway misc.sendmail\n\nskyway slurm --update-conf\n</code></pre> <p>Some notes:</p> <ul> <li>Enable Skyway as a trusted operator:  Is it really possible from Skyway setting, or needs some sort of ITS support? \u2013 PIs ask ITS to use RCC Skyway to operate their AWS account: ITS is supposed to have a script that gives RCC permission to operate the given AWS account through their organizational AWS master access key.</li> <li>Suppose that I want to add another instance type to AWS and to GCP, what should I do? \u2013 see <code>/opt/skyway/etc/cloud.yml</code></li> <li>Suppose that I want to add another cloud account (AWS and GCP) for a PI, what is the protocol and how can I achieve it from the configuration file? \u2013 add another yaml file under <code>/opt/skyway/etc/accounts</code></li> <li>If I change the source code under <code>/opt/skyway/pkgs/skyway</code>, what are the steps to test the changes? no need to stop and restart the daemons, everything is python.</li> </ul>"},{"location":"user_overview/","title":"User Guide","text":"<p>This documentation briefly explains how regular users access to Skyway and submit jobs to use cloud services. Please refer to the Skyway home page for more information and news.</p>"},{"location":"user_overview/#gaining-access","title":"Gaining Access","text":"<p>You first need an active RCC User account (see accounts and allocations page). Next, you should contact your PI or class instructors for access to Skyway. Alternatively, you can reach out to our Help Desk at help@rcc.uchicago.edu for assistance.</p>"},{"location":"user_overview/#connecting","title":"Connecting","text":"<p>Once your RCC User account is active, you log in to the Midway cluster with your <code>CNetID</code> <pre><code>  ssh [CNetID]@midway2.rcc.uchicago.edu\n</code></pre> then log in to Skyway from Midway2: <pre><code>  ssh [CNetID]@skyway.rcc.uchicago.edu\n</code></pre> If successful, you will get access to the Skyway login node, where you can access to the following locations:</p> <ol> <li><code>/home/[CNetID]</code> This is the temporary home directory (no backup) for users on Skyway. Note, this is NOT the home file system on Midway, so you won\u2019t see any contents from your home directory on midway. Please do NOT store any sizable or important data here. (<code>TODO</code>: Add note here about changing $HOME environment variable to <code>/cloud/aws/[CNetID]</code>.)</li> <li><code>/project</code> and <code>/project2</code> This is the RCC high-performance capacity storage file systems from Midway, mounted on Skyway, with the same quotas and usages as on Midway. Just as with running jobs on Midway, /project and /project2 should be treated as the location for users to store the data they intend to keep. This also acts as a way to make data accessible between Skyway and midway as the /project and /project2 filesystems are mounted on both systems. Run <code>cd /project/&lt;labshare&gt;</code> or <code>/project2/&lt;labshare&gt;</code>, where <code>&lt;labshare&gt;</code> is the name of the lab account, to access the files by your lab or group. This will work even if the lab share directory does not appear in a file listing, e.g., <code>ls /project</code>.</li> <li><code>/cloud/[cloud]/[CNetID]</code> Options of [cloud]: aws or gcp This is the cloud scratch folder (no backup), which is intended for read/write of cloud compute jobs. For example, with Amazon cloud resources (AWS) The remote cloud S3 AWS bucket storage is mounted to Skyway at this path. Before submitting jobs to the cloud compute resources, users must first stage the data, scripts and executables their cloud job will use to the /cloud/aws/[CNetID] folder. After running their cloud compute job, users should then copy the data they wish to keep from the /cloud/aws/[CNetID] folder back to their project folder. Similarly, if users are using Google Cloud Platform (GCP), the scratch folder /cloud/gcp/[CNetID] should be used.</li> </ol> <p>You can create your own folders, upload data, write and compile codes, prepare job scripts and submit jobs in a similar manner to what you do on Midway.</p> <p>Skyway provides compiled software packages (i.e. <code>modules</code>) that you can load to build your codes or run your jobs. The list of the modules is given in the Skyway home page.</p>"},{"location":"user_overview/#running-jobs","title":"Running Jobs","text":"<p>You submit jobs to SLURM in a similar manner to what do on Midway. The difference is that you should specify different partitions and accounts corresponding to the cloud services you have access to (e.g. AWS or GCP). Additionally, the instance configuration should be specified via <code>--constraint</code>.</p> <p>To submit jobs to cloud, you must specify a type of virtual machine (VM) by the option <code>--constraint=[VM Type]</code>. The VM types currently supported through Skyway can be found in the table below. You can also get an up-to-date listing of the machine types by running command sinfo-node-types on a skyway login node.</p> VM Type Description AWS EC2 Instance t1 1 core, 1G Mem (for testing and building software) t2.micro c1 1 core, 4G Mem (for serial jobs) c5.large c8 8 cores, 32G Mem (for medium sized multicore jobs) c5.4xlarge c36 36 cores, 144G Mem (for large memory jobs) c5.18xlarge m24 24 cores, 384G Mem (for large memory jobs) c5.12xlarge g1 1x V100 GPU (for GPU jobs) p3.2xlarge g4 4x V100 GPU (for heavy GPU jobs) p3.8xlarge g8 8x V100 GPU (for heavy GPU jobs) p3.16xlarge <p>When submitting jobs, include following two options in the job script:</p> <ul> <li><code>--partition=rcc-aws</code></li> <li><code>--account=rcc-aws</code></li> </ul> <p>Some commonly used SLURM commands are:</p> <ul> <li>sinfo \u2013 Show compute nodes status</li> <li>sbatch \u2013 Submit computing jobs</li> <li>scancel \u2013 Cancel submitted jobs</li> <li>sacct \u2013 Check logs of recent jobs</li> </ul> <p>A sample job script <code>sample.sbatch</code> would look like this</p> <pre><code>#!/bin/sh\n\n#SBATCH --job-name=TEST\n#SBATCH --partition=rcc-aws\n#SBATCH --account=rcc-aws\n#SBATCH --exclusive\n#SBATCH --ntasks=1\n#SBATCH --constraint=t2 # Specifies you would like to use a t2 instance\n\ncd $SLURM_SUBMIT_DIR\n\nhostname\nlscpu\nlscpu --extended\nfree -h\n</code></pre> <p>You can also request interactive jobs for testing and debugging purpuses: <pre><code>   sinteractive --partition=rcc-aws --constraint=t2 --ntasks=1\n</code></pre> and with GPUs <pre><code>   sinteractive --partition=rcc-aws --constraint=g1 --ntasks=1 --gres=gpu:1\n</code></pre></p>"},{"location":"user_overview/#troubleshooting","title":"Troubleshooting","text":"<p>For further assistance, please contact our Help Desk at help@rcc.uchicago.edu.</p>"}]}